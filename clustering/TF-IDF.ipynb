{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "mypath = \"./InputFiles\"\n",
    "mypath2 = \"./TestFiles\"\n",
    "files = [join(mypath2, f) for f in listdir(mypath2) if isfile(join(mypath2, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing of the texts\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, PorterStemmer())\n",
    "    return stems\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print 1\n",
    "\n",
    "nrfiles = 100\n",
    "token_dict = {}\n",
    "\n",
    "#print files\n",
    "for file in files[:nrfiles]:\n",
    "    text = unicode(open(file).read(), errors='replace')\n",
    "    lowers = text.lower()\n",
    "    \n",
    "    #print lowers\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    no_punctuation = ''.join(ch for ch in lowers if ch not in exclude)\n",
    "    token_dict[file] = no_punctuation\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize /,stop_words='english').fit_transform(token_dict.values())\n",
    "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
    "similarity = tfidf * tfidf.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build graph of similarities using the below threshold\n",
    "threshold = 0.20\n",
    "arr = similarity.toarray()\n",
    "\n",
    "rownr = 0\n",
    "colnr = 0\n",
    "w = h = 10\n",
    "\n",
    "\n",
    "graph = [list() for i in range(nrfiles)]\n",
    "\n",
    "for rowi, row in enumerate(arr):\n",
    "    for eli, el in enumerate(row):\n",
    "        if rowi != eli and el >= threshold:\n",
    "            graph[rowi].append(eli)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[[0, 39, 5, 27, 6, 28, 35, 2, 36, 3, 54, 40, 17, 43, 46, 13, 37, 52, 8, 21, 24, 25, 7, 71, 22, 62, 56, 77, 53, 29, 61, 45, 26, 42, 47, 34, 91, 57, 48, 55, 51, 59, 79, 78, 84, 92, 69, 64, 96, 95, 99, 4, 85, 87, 68, 44, 88, 70, 66, 16, 50, 93, 82, 76, 81, 58, 60, 30], [1, 97], [9, 41, 49], [10, 38], [11], [12, 33], [14], [15], [18, 72, 98], [19], [20, 65], [23], [31], [32], [63], [67], [73, 86], [74], [75], [80], [83], [89], [90], [94], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "#compute the number of unique events -> number of connected components\n",
    "#mapping between clusters/events and the sources\n",
    "clusters = [list() for i in range(nrfiles)]\n",
    "\n",
    "def dfs(node, component):\n",
    "    visited[node] = True\n",
    "    clusters[component].append(node)\n",
    "    \n",
    "    for neib in graph[node]:\n",
    "        if neib not in visited:\n",
    "            dfs(neib, component)\n",
    "\n",
    "visited = {}\n",
    "components = 0   \n",
    "for idx, node in enumerate(graph):\n",
    "    if idx not in visited:\n",
    "        dfs(idx, components)\n",
    "        components = components + 1\n",
    "print components\n",
    "\n",
    "        \n",
    "print clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
